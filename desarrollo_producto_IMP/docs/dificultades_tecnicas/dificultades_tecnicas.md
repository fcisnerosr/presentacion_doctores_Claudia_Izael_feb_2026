# Dificultades T√©cnicas - Procesador de Fatiga SACS v1.0

## Documento de An√°lisis T√©cnico

**Proyecto**: Procesador de Fatiga SACS v1.0  
**Fecha**: 04 de Febrero, 2026  
**Prop√≥sito**: Identificar, documentar y resolver los desaf√≠os t√©cnicos del parsing de archivos SACS FTG

---

## üî¥ DIFICULTADES CR√çTICAS

### 1. PARSING DE CAMPO MEMBER (Variable Format)

#### **¬øQu√© es?**
El campo MEMBER en los archivos SACS presenta m√∫ltiples formatos inconsistentes que dificultan su extracci√≥n confiable.

#### **¬øC√≥mo se manifiesta?**
Ejemplos encontrados en el archivo:
```
JOINT   MEMBER      GRUP
0003    802L 0005   16A      ‚Üê Formato con espacio
0002    0002-501L   52A      ‚Üê Formato con gui√≥n
0002    401L-0002   52A      ‚Üê Formato invertido con gui√≥n
0003    802L-0003   DL9      ‚Üê Variante adicional
```

El campo MEMBER puede contener:
- Espacios internos (`802L 0005`)
- Guiones en diferentes posiciones (`0002-501L`, `401L-0002`)
- Longitud variable (9-11 caracteres)
- Combinaciones de n√∫meros y letras sin patr√≥n fijo

#### **¬øPor qu√© es un problema?**
1. **Split por espacios falla**: Usar `line.split()` separar√≠a `"802L 0005"` en dos campos distintos
2. **Posiciones fijas inciertas**: Sin conocer el ancho exacto de columna, el parsing por substring puede truncar datos
3. **Identificaci√≥n √∫nica comprometida**: Si MEMBER se parsea incorrectamente, la llave √∫nica `JOINT_MEMBER_GRUP` ser√° inv√°lida
4. **Colisi√≥n de claves**: Parseos inconsistentes pueden generar claves duplicadas o faltantes al cruzar archivos

#### **¬øPara qu√© necesitamos resolverlo?**
- **Integridad de datos**: MEMBER es parte de la llave √∫nica para identificar cada elemento estructural
- **Suma correcta entre archivos**: Si las claves son inconsistentes, los da√±os no se sumar√°n correctamente
- **Trazabilidad**: Los ingenieros necesitan identificar exactamente qu√© miembro estructural tiene qu√© da√±o acumulado

#### **Estrategia de soluci√≥n**
```python
# Opci√≥n 1: Regex con grupos nombrados
pattern = r'^(?P<joint>\w+)\s+(?P<member>\S+(?:\s+\S+)?)\s+(?P<grup>\w+)'
match = re.match(pattern, line)
joint = match.group('joint')
member = match.group('member')
grup = match.group('grup')

# Opci√≥n 2: Columnas fijas (requiere validaci√≥n en m√∫ltiples archivos)
joint = line[0:6].strip()
member = line[6:20].strip()  # Ancho aumentado para capturar espacios
grup = line[20:25].strip()

# Opci√≥n 3: Split inteligente
parts = line.split()
joint = parts[0]
grup = parts[-1]  # √öltimo elemento siempre es GRUP
member = ' '.join(parts[1:-1])  # Todo lo que est√° entre JOINT y GRUP
```

**Validaci√≥n necesaria**: Comparar resultados de las tres t√©cnicas en 100 l√≠neas aleatorias del archivo.

---

### 2. NOTACI√ìN CIENT√çFICA FORTRAN (Sin "E" expl√≠cita)

#### **¬øQu√© es?**
SACS exporta n√∫meros en notaci√≥n cient√≠fica Fortran donde el exponente se escribe sin la letra "E" y sin el "0" inicial, causando que Python no pueda convertirlos a `float()`.

#### **¬øC√≥mo se manifiesta?**
Ejemplos encontrados:
```
FORMATO FORTRAN              FORMATO ESPERADO
.48430268-9                  0.48430268E-9
.10756032-8                  0.10756032E-8
.58233452-8                  0.58233452E-8
```

Patr√≥n identificado: `.<d√≠gitos><signo><exponente>`

#### **¬øPor qu√© es un problema?**
1. **Error de conversi√≥n**: Python lanza `ValueError: could not convert string to float: '.48430268-9'`
2. **Distribuci√≥n masiva**: Encontrado en m√°s de 20 ocurrencias en la secci√≥n "MEMBER LOAD-CASE DAMAGE REPORT" (l√≠nea 93050+)
3. **P√©rdida de datos**: Sin normalizaci√≥n, todos los valores en formato Fortran se perder√≠an
4. **Coexistencia de formatos**: En el mismo archivo hay valores en formato est√°ndar (`0.817300E-05`) y Fortran (`.48430268-9`)

#### **¬øPara qu√© necesitamos resolverlo?**
- **C√°lculos num√©ricos**: Los valores de da√±o deben convertirse a float para sumarlos aritm√©ticamente
- **Precisi√≥n**: Los valores de fatiga son t√≠picamente muy peque√±os (1E-7 a 1E-10), cualquier p√©rdida de precisi√≥n es inaceptable
- **Compatibilidad**: Pandas y NumPy requieren tipos num√©ricos v√°lidos para operaciones

#### **Estrategia de soluci√≥n**
```python
import re

def normalize_fortran_scientific(value_str):
    """
    Convierte notaci√≥n cient√≠fica Fortran a formato Python est√°ndar.
    
    Transformaciones:
    - .123-4  ‚Üí 0.123E-04
    - .123+4  ‚Üí 0.123E+04
    - 1.23-4  ‚Üí 1.23E-04  (caso adicional sin 0 inicial)
    
    Args:
        value_str (str): Valor en formato Fortran
        
    Returns:
        float: Valor num√©rico convertido
    """
    # Caso 1: .d√≠gitos-exponente o .d√≠gitos+exponente
    value_str = re.sub(r'(\.\d+)([+-])(\d+)', r'0\1E\2\3', value_str)
    
    # Caso 2: d√≠gitos.d√≠gitos-exponente (sin E)
    value_str = re.sub(r'(\d+\.\d+)([+-])(\d+)', r'\1E\2\3', value_str)
    
    # Caso 3: d√≠gitos-exponente (entero sin decimal)
    value_str = re.sub(r'(\d+)([+-])(\d+)$', r'\1E\2\3', value_str)
    
    try:
        return float(value_str)
    except ValueError as e:
        raise ValueError(f"No se pudo convertir '{value_str}' despu√©s de normalizaci√≥n: {e}")

# Aplicaci√≥n en l√≠nea de datos
damage_values = []
for value_str in line.split()[4:12]:  # 8 valores de da√±o por l√≠nea
    normalized_value = normalize_fortran_scientific(value_str)
    damage_values.append(normalized_value)
```

**Casos de prueba**:
```python
assert normalize_fortran_scientific('.48430268-9') == 0.48430268e-9
assert normalize_fortran_scientific('.10756032-8') == 0.10756032e-8
assert normalize_fortran_scientific('0.817300E-05') == 0.817300e-5
```

---

### 3. SUMA ARITM√âTICA ENTRE M√öLTIPLES ARCHIVOS

#### **¬øQu√© es?**
SACS genera un archivo de fatiga por etapa temporal (ej. a√±os 0-10, 10-20, 20-30). El software **NO** suma autom√°ticamente el da√±o acumulado entre modelos, por lo que debemos hacerlo manualmente.

#### **¬øC√≥mo se manifiesta?**
Estructura de archivos de entrada:
```
data/
  ‚îú‚îÄ‚îÄ ftglstE1.txt  (a√±os 0-10)
  ‚îú‚îÄ‚îÄ ftglstE2.txt  (a√±os 10-20)
  ‚îî‚îÄ‚îÄ ftglstE3.txt  (a√±os 20-30)
```

Cada archivo contiene:
```
JOINT  MEMBER    GRUP    TOP      TOP-LEFT  ...  TOP-RIGHT
0003   802L 0005  16A   0.00081   0.00073   ...  0.00036
```

Salida esperada (suma):
```
JOINT  MEMBER    GRUP    TOP           TOP-LEFT      ...
0003   802L 0005  16A   0.00081*3     0.00073*3     ...  (suma de 3 archivos)
```

#### **¬øPor qu√© es un problema?**
1. **Claves inconsistentes**: Si un elemento aparece en E1 pero no en E2, la suma debe manejar valores faltantes
2. **Orden de archivos**: El usuario puede seleccionar archivos en cualquier orden
3. **Volumen de datos**: Con ~2000 elementos por archivo √ó 3 archivos = 6000 registros a consolidar
4. **Validaci√≥n de integridad**: ¬øQu√© hacer si un JOINT-MEMBER aparece en E1 pero no en E3?

#### **¬øPara qu√© necesitamos resolverlo?**
- **Requisito central del negocio**: El objetivo del software ES sumar da√±os entre etapas
- **Cumplimiento normativo**: Los c√≥digos de dise√±o (API RP-2A, DNV) requieren da√±o acumulado total
- **Decisiones de ingenier√≠a**: Los ingenieros necesitan saber si un elemento excede el l√≠mite de vida √∫til (damage > 1.0)

#### **Estrategia de soluci√≥n**
```python
from collections import defaultdict
import pandas as pd
import numpy as np

def process_multiple_files(file_paths, progress_callback=None):
    """
    Procesa m√∫ltiples archivos SACS y suma da√±os acumulados.
    
    Args:
        file_paths (list): Lista de rutas de archivos .txt
        progress_callback (callable): Funci√≥n para actualizar barra de progreso
        
    Returns:
        pd.DataFrame: DataFrame con da√±os consolidados
    """
    # Estructura: {(joint, member, grup): [top, top_left, ..., top_right]}
    accumulated_damage = defaultdict(lambda: np.zeros(8, dtype=np.float64))
    
    total_files = len(file_paths)
    
    for i, filepath in enumerate(file_paths):
        # Parsear archivo individual
        file_data = parse_fatigue_file(filepath)
        
        # Acumular da√±os
        for key, damages in file_data.items():
            accumulated_damage[key] += damages
        
        # Actualizar progreso
        if progress_callback:
            progress_callback(i + 1, total_files)
    
    # Convertir a DataFrame
    df = pd.DataFrame.from_dict(
        accumulated_damage,
        orient='index',
        columns=['TOP', 'TOP-LEFT', 'LEFT', 'BOT-LEFT', 
                'BOT', 'BOT-RIGHT', 'RIGHT', 'TOP-RIGHT']
    )
    
    # Extraer JOINT, MEMBER, GRUP del √≠ndice
    df.reset_index(inplace=True)
    df[['JOINT', 'MEMBER', 'GRUP']] = df['index'].str.split('_', n=2, expand=True)
    df.drop('index', axis=1, inplace=True)
    
    # Calcular da√±o m√°ximo por fila
    damage_columns = ['TOP', 'TOP-LEFT', 'LEFT', 'BOT-LEFT', 
                     'BOT', 'BOT-RIGHT', 'RIGHT', 'TOP-RIGHT']
    df['MAX_DAMAGE'] = df[damage_columns].max(axis=1)
    df['CRITICAL_LOCATION'] = df[damage_columns].idxmax(axis=1)
    
    # Ordenar por da√±o m√°ximo (descendente)
    df.sort_values('MAX_DAMAGE', ascending=False, inplace=True)
    
    return df

# Manejo de elementos faltantes
# Si un elemento NO aparece en un archivo, su contribuci√≥n es 0 (comportamiento de defaultdict)
# Esto es correcto: si no est√° en el modelo, no aporta da√±o en esa etapa
```

**Validaci√≥n**: Comparar suma manual de 5 elementos contra resultado del software.

---

### 4. L√ìGICA DE ESTADO MULTIL√çNEA (State Machine)

#### **¬øQu√© es?**
Cada bloque de datos de un elemento estructural se distribuye en **18 l√≠neas**:
- 1 l√≠nea de encabezado (JOINT, MEMBER, GRUP, LOAD CASE 1)
- 15 l√≠neas adicionales (LOAD CASE 2-16)
- 1 l√≠nea de resumen (`*** TOTAL DAMAGE ***`)

#### **¬øC√≥mo se manifiesta?**
```
L√≠nea N:   0003  802L 0005  16A   1  0.00993  0.01582  ...
L√≠nea N+1:                         2  0.02721  0.04682  ...
L√≠nea N+2:                         3  0.16695  0.22870  ...
...
L√≠nea N+15:                       16  0.03047  0.03275  ...
L√≠nea N+16:    *** TOTAL DAMAGE ***   0.817E-05 0.727E-05 ...
```

No hay un marcador claro de fin de bloque **excepto** la l√≠nea `*** TOTAL DAMAGE ***`.

#### **¬øPor qu√© es un problema?**
1. **Parser secuencial**: Debemos mantener estado entre l√≠neas para saber si estamos leyendo casos de carga o el total
2. **Saltos de p√°gina**: Los encabezados de p√°gina pueden aparecer **dentro** de un bloque de 18 l√≠neas
3. **Detecci√≥n de inicio**: ¬øC√≥mo saber cu√°ndo inicia un nuevo bloque? (JOINT en columna 0)
4. **Bloques incompletos**: ¬øQu√© hacer si un archivo termina sin la l√≠nea `*** TOTAL DAMAGE ***`?

#### **¬øPara qu√© necesitamos resolverlo?**
- **Extracci√≥n correcta**: Solo la l√≠nea `*** TOTAL DAMAGE ***` contiene los valores que debemos sumar
- **Robustez**: El parser no debe "perderse" si hay l√≠neas inesperadas
- **Debugging**: Saber en qu√© estado est√° el parser ayuda a diagnosticar errores

#### **Estrategia de soluci√≥n**
```python
from enum import Enum

class ParserState(Enum):
    SEARCHING = 1      # Buscando inicio de secci√≥n MEMBER FATIGUE REPORT
    READING_HEADER = 2 # Leyendo encabezado de columnas
    READING_CASES = 3  # Leyendo 16 casos de carga
    READING_TOTAL = 4  # Esperando l√≠nea *** TOTAL DAMAGE ***

def parse_fatigue_file(filepath):
    """
    Parser con m√°quina de estados para archivos SACS FTG.
    """
    state = ParserState.SEARCHING
    current_key = None
    current_cases = []
    results = {}
    
    with open(filepath, 'r', encoding='latin-1') as f:
        for line_num, line in enumerate(f, start=1):
            
            # Ignorar l√≠neas de encabezado/p√°gina
            if 'SACS (2024)' in line or 'FTG PAGE' in line:
                continue
            
            # Estado: Buscando secci√≥n
            if state == ParserState.SEARCHING:
                if 'MEMBER FATIGUE DETAIL REPORT' in line:
                    state = ParserState.READING_HEADER
                    continue
            
            # Estado: Leyendo encabezado de columnas
            elif state == ParserState.READING_HEADER:
                if 'JOINT' in line and 'MEMBER' in line and 'GRUP' in line:
                    state = ParserState.READING_CASES
                    continue
            
            # Estado: Leyendo casos de carga
            elif state == ParserState.READING_CASES:
                # ¬øEs inicio de nuevo bloque? (JOINT en columna 0)
                if line[0:6].strip() and not line.strip().startswith('***'):
                    # Extraer JOINT, MEMBER, GRUP
                    parts = line.split()
                    joint = parts[0]
                    grup = parts[-9]  # GRUP est√° 9 posiciones antes del final
                    member = ' '.join(parts[1:-9])
                    current_key = f"{joint}_{member}_{grup}"
                    current_cases = []
                
                # ¬øEs l√≠nea *** TOTAL DAMAGE ***?
                if '*** TOTAL DAMAGE ***' in line:
                    # Extraer 8 valores de da√±o
                    values = line.split()[3:11]  # Despu√©s de "*** TOTAL DAMAGE ***"
                    damage_array = np.array([
                        normalize_fortran_scientific(v) for v in values
                    ])
                    results[current_key] = damage_array
                    current_key = None
    
    return results
```

**Mejora futura**: A√±adir logging para diagnosticar bloques incompletos.

---

## üü° DIFICULTADES MEDIAS

### 5. M√öLTIPLES REGISTROS POR JOINT (Relaci√≥n 1:N)

#### **¬øQu√© es?**
Un mismo JOINT puede tener m√∫ltiples MEMBER asociados (conexiones estructurales).

#### **¬øC√≥mo se manifiesta?**
```
JOINT  MEMBER       GRUP
0003   802L 0005    16A     ‚Üê Conexi√≥n 1
0003   0003-0005    16A     ‚Üê Conexi√≥n 2
0003   802L-0003    DL9     ‚Üê Conexi√≥n 3
```

#### **¬øPor qu√© es un problema?**
Si usamos solo `JOINT` como clave primaria, sobrescribiremos registros.

#### **¬øPara qu√© necesitamos resolverlo?**
Cada conexi√≥n tiene su propio da√±o acumulado independiente. Perder registros significa p√©rdida de informaci√≥n cr√≠tica de seguridad estructural.

#### **Soluci√≥n**
Usar clave compuesta: `JOINT + MEMBER + GRUP`

```python
key = f"{joint}_{member}_{grup}"
```

---

### 6. ARCHIVOS GRANDES (Performance)

#### **¬øQu√© es?**
El archivo de prueba tiene **146,370 l√≠neas** (~12 MB). Archivos de producci√≥n pueden ser mucho mayores.

#### **¬øPor qu√© es un problema?**
- Cargar todo en memoria puede causar `MemoryError`
- Procesamiento lento afecta experiencia de usuario
- GUI puede "congelarse" sin multi-threading

#### **¬øPara qu√© necesitamos resolverlo?**
La aplicaci√≥n debe procesar m√∫ltiples archivos grandes en tiempo razonable (<30 segundos para 3 archivos de 150k l√≠neas).

#### **Soluci√≥n**
```python
# 1. Procesamiento l√≠nea por l√≠nea (streaming)
def parse_fatigue_file(filepath):
    with open(filepath, 'r') as f:
        for line in f:  # No carga todo en RAM
            process_line(line)

# 2. Threading para GUI
from threading import Thread

def process_files_threaded(files, callback):
    thread = Thread(target=process_files, args=(files, callback))
    thread.start()
```

---

## üü¢ DIFICULTADES BAJAS

### 7. SALTOS DE P√ÅGINA

#### **¬øQu√© es?**
Encabezados repetitivos cada ~50 l√≠neas:
```
SACS (2024)                         FTG PAGE  810
* *  M E M B E R  F A T I G U E  D E T A I L  R E P O R T  * *
```

#### **Soluci√≥n**
```python
if 'SACS (2024)' in line or 'FTG PAGE' in line:
    continue
```

---

### 8. ESPACIOS EN BLANCO VARIABLES

#### **¬øQu√© es?**
Separaci√≥n de valores con espacios de longitud variable.

#### **Soluci√≥n**
```python
values = line.split()  # Divide por cualquier whitespace
```

---

## Matriz de Riesgo

| Dificultad | Impacto | Probabilidad | Prioridad |
|------------|---------|--------------|-----------|
| Parsing MEMBER | Alto | Alta | P0 |
| Notaci√≥n Fortran | Alto | Alta | P0 |
| Suma entre archivos | Alto | Alta | P0 |
| State Machine | Medio | Media | P1 |
| M√∫ltiples registros | Medio | Alta | P1 |
| Archivos grandes | Medio | Media | P2 |
| Saltos de p√°gina | Bajo | Alta | P3 |
| Espacios variables | Bajo | Baja | P3 |

---

## Recomendaciones de Testing

1. **Unit Tests**: Crear casos de prueba para cada funci√≥n de parsing
2. **Integration Tests**: Probar suma de 2-3 archivos reales
3. **Edge Cases**: Archivos vac√≠os, bloques incompletos, valores extremos
4. **Performance Tests**: Medir tiempo con archivos de 200k+ l√≠neas
5. **Validation**: Comparar resultados contra c√°lculo manual en Excel

---

**Documento generado**: 04/02/2026  
**Revisi√≥n**: v1.0  
**Autor**: An√°lisis t√©cnico automatizado
